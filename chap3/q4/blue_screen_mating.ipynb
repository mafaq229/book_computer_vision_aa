{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 3.4: Blue screen matting. Set up a blue or green background, e.g., by buying a large\n",
    "piece of colored posterboard. Take a picture of the empty background, and then of the back-\n",
    "ground with a new object in front of it. Pull the matte using the difference between each\n",
    "colored pixel and its assumed corresponding background pixel, using one of the techniques\n",
    "described in Section 3.1.3 or by Smith and Blinn (1996). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue screen (or green screen) matting is a common technique in photography and film to isolate a foreground subject from a uniform background. Once isolated, the foreground can be composited (“placed”) into a new scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a practical, step-by-step guide:\n",
    "\n",
    "### Capture Images:\n",
    "Background image (IbgIbg​): Photograph the poster board (blue/green) by itself, making sure the lighting is consistent with your foreground shots.\n",
    "Foreground image (IfgIfg​): Place your object (or person) in front of the same background and capture the photo.\n",
    "\n",
    "### Read Images into a Program:\n",
    "For instance, use Python with OpenCV:\n",
    "\n",
    "import cv2\n",
    "bg_img = cv2.imread('background.jpg')  # The empty background\n",
    "fg_img = cv2.imread('foreground.jpg')  # Foreground with background\n",
    "\n",
    "Ensure both images have the same resolution and align properly.\n",
    "\n",
    "### Preprocess Images (Optional):\n",
    "\n",
    "Adjust brightness/contrast, or apply slight blurring to the background image to handle minor noise.\n",
    "If camera alignment was not perfect, you may need to align or register the images (e.g., using feature matching or manual alignment).\n",
    "\n",
    "### Compute the Difference and Threshold:\n",
    "\n",
    "Convert both images into the same color space (e.g., BGR or HSV).\n",
    "For each pixel xx:\n",
    "Compute the color difference d(x)d(x).\n",
    "Compare d(x)d(x) to one or more thresholds (TlowTlow​, ThighThigh​) to assign α(x)α(x).\n",
    "\n",
    "### Create the Matte (αα-mask):\n",
    "\n",
    "Store the αα values in a matrix of the same size as your images.\n",
    "This αα-mask can be a float image ranging from 0 to 1 (for smooth transitions).\n",
    "\n",
    "### (Optional) Foreground Color Estimation:\n",
    "\n",
    "If you want a more accurate foreground color F(x)F(x) (especially near edges), you can estimate:\n",
    "F(x)=Ifg(x)−(1−α(x)) B(x)α(x),\n",
    "F(x)=α(x)Ifg​(x)−(1−α(x))B(x)​, but only where α(x)≠0α(x)=0.\n",
    "\n",
    "### Composite the Foreground onto a New Background (Optional Extension):\n",
    "\n",
    "Let N(x)N(x) be a new background image at pixel xx.\n",
    "The compositing equation is:\n",
    "Ifinal(x)=α(x) F(x)  +  (1−α(x)) N(x).\n",
    "Ifinal​(x)=α(x)F(x)+(1−α(x))N(x).\n",
    "This step is useful if you want to place your foreground object into a different scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the background (empty) and foreground images\n",
    "bg_img = cv2.imread('background.png')\n",
    "fg_img = cv2.imread('foreground.png')\n",
    "\n",
    "# Convert images to float for more precise calculations\n",
    "bg_float = bg_img.astype(np.float32) / 255.0\n",
    "fg_float = fg_img.astype(np.float32) / 255.0\n",
    "\n",
    "# 1. Compute the color difference (Euclidean distance in BGR)\n",
    "diff = np.sqrt(np.sum((fg_float - bg_float)**2, axis=2))\n",
    "\n",
    "# 2. Define thresholds (tune these empirically)\n",
    "T_low = 0.05   # below this -> definitely background\n",
    "T_high = 0.15  # above this -> definitely foreground\n",
    "\n",
    "# 3. Initialize alpha matte\n",
    "alpha = np.zeros_like(diff)\n",
    "\n",
    "# 4. Hard thresholding for demonstration:\n",
    "alpha[diff < T_low] = 0.0\n",
    "alpha[diff > T_high] = 1.0\n",
    "\n",
    "# 5. Optional: Smooth transition in the [T_low, T_high] range\n",
    "transition_region = np.where((diff >= T_low) & (diff <= T_high))\n",
    "alpha[transition_region] = (diff[transition_region] - T_low) / (T_high - T_low)\n",
    "\n",
    "# alpha is now in [0,1]\n",
    "\n",
    "# 6. (Optional) Estimate the foreground color F in edge/transition regions\n",
    "#    We'll do a direct substitution for demonstration, but in real use you want\n",
    "#    to avoid division by zero. Only do this where alpha > 0.\n",
    "F = np.zeros_like(fg_float)\n",
    "non_zero_alpha = alpha > 0.001\n",
    "F[non_zero_alpha] = (fg_float[non_zero_alpha] - (1 - alpha[non_zero_alpha, None]) * bg_float[non_zero_alpha]) \\\n",
    "                    / alpha[non_zero_alpha, None]\n",
    "\n",
    "# 7. (Optional) Composite onto a new background\n",
    "new_bg = cv2.imread('new_background.png').astype(np.float32)/255.0\n",
    "new_bg = cv2.resize(new_bg, (bg_img.shape[1], bg_img.shape[0]))  # Resize if needed\n",
    "\n",
    "# The final composited image\n",
    "I_final = F * alpha[..., None] + new_bg * (1.0 - alpha[..., None])\n",
    "\n",
    "# Convert alpha and I_final back to 8-bit for visualization/saving\n",
    "alpha_8u = (alpha * 255).astype(np.uint8)\n",
    "I_final_8u = np.clip(I_final * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Save or display the results\n",
    "cv2.imwrite('alpha.png', alpha_8u)\n",
    "cv2.imwrite('final_composited.png', I_final_8u)\n",
    "\n",
    "cv2.imshow('Alpha Matte', alpha_8u)\n",
    "cv2.imshow('Composited Image', I_final_8u)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Notes on the code:**\n",
    "- We load **two images**: the background alone and the foreground.  \n",
    "- We compute a **difference map** (`diff`) to measure how different each pixel is from the known background.  \n",
    "- We use **two thresholds** to determine whether a pixel is definitely background, definitely foreground, or in a transitional region.  \n",
    "- We allow for a **smooth alpha ramp** between \\(\\alpha = 0\\) and \\(\\alpha = 1\\) in that transitional region. This helps reduce sharp edges in the matte.  \n",
    "- Finally, we composite the extracted foreground onto a **new background** (optional step) using the standard compositing formula:\n",
    "  \\[\n",
    "  I_\\text{final} = \\alpha \\cdot F + (1-\\alpha) \\cdot N.\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Practical Considerations**\n",
    "\n",
    "1. **Lighting Conditions:**  \n",
    "   Ensure that your background is lit **evenly** to avoid large intensity gradients. Any uneven lighting will reduce the effectiveness of simple difference or color keying.\n",
    "\n",
    "2. **Shadows & Color Spill:**  \n",
    "   Shadows on the background can lead to increased difference. Similarly, color from the background can spill onto the foreground (particularly with green screens and reflective surfaces). These may require more advanced spill-suppression or shadow-removal techniques.\n",
    "\n",
    "3. **Image Alignment:**  \n",
    "   If the camera or background moved slightly between the background and foreground shots, you need to **align** the images or capture them from a fixed camera setup.\n",
    "\n",
    "4. **Threshold Tuning:**  \n",
    "   You may need to experiment with threshold values \\((T_\\text{low}, T_\\text{high})\\) for best results. Adjusting them can drastically change the quality of the matte.\n",
    "\n",
    "5. **More Advanced Keying Methods:**  \n",
    "   Professional keying methods (e.g., **chroma keying**, **Bayesian matting**, **closed-form matting**) use more sophisticated color models and priors to handle transparency, hair, motion blur, etc. The difference keying shown here is a simplified approach but is sufficient for an introductory exercise.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "By capturing two images (background-only and foreground-in-place) and comparing their pixel differences, you can derive a **matte** (\\(\\alpha\\)) that identifies foreground vs. background. This technique (sometimes called **difference keying**) is a straightforward application of the matting equation. Once the matte is obtained, you can optionally refine the **foreground colors** and then **composite** your object onto any desired new background.\n",
    "\n",
    "**Key references:**\n",
    "- Szeliski, *Computer Vision: Algorithms and Applications*, Section 3.1.3.\n",
    "- Smith and Blinn (1996), *Blue Screen Matting* in SIGGRAPH.\n",
    "\n",
    "Feel free to extend this code with more robust methods for shadow detection, spill suppression, and refined color estimation to get cleaner results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
