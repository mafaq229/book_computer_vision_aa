{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 3.5: Difference keying. Implement a difference keying algorithm (see Section 3.1.3)\n",
    "(Toyama, Krumm et al. 1999), consisting of the following steps:\n",
    "1. Compute the mean and variance (or median and robust variance) at each pixel in an\n",
    "“empty” video sequence.\n",
    "2. For each new frame, classify each pixel as foreground or background (set the back-\n",
    "ground pixels to RGBA=0).\n",
    "3. (Optional) Compute the alpha channel and composite over a new background.\n",
    "4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\n",
    "components (Section 3.3.3), compute their centroids, and track them from frame to\n",
    "frame. Use this to build a “people counter”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key points in the code above:**\n",
    "1. We read **30 frames** of an empty scene to compute our background model (mean & variance).  \n",
    "2. For each **new frame**, we compute the **difference** from the background mean, then compare with a threshold based on the standard deviation of the background.  \n",
    "3. We use **morphological operations** (`cv2.morphologyEx`) to clean up the foreground mask.  \n",
    "4. We use **connected component labeling** (`cv2.connectedComponents`) to detect blobs, draw bounding boxes, and find centroids.  \n",
    "5. For an advanced “people counter,” you could track these centroids over time, counting how many unique objects appear and possibly exit the scene.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Practical Considerations**\n",
    "\n",
    "1. **Illumination Changes:**  \n",
    "   - Sudden lighting changes can break a simple mean/variance model. Consider more robust or adaptive background modeling (e.g., **Gaussian Mixture Models**, [OpenCV’s `BackgroundSubtractorMOG2`](https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html)).\n",
    "\n",
    "2. **Static vs. Dynamic Background:**  \n",
    "   - If the background is not truly static (e.g., waving trees), a single mean/variance might not be sufficient. Multiple model components or motion compensation might be needed.\n",
    "\n",
    "3. **Threshold Parameter \\(k\\):**  \n",
    "   - The choice of \\(k\\) directly affects how sensitive the detection is. A smaller \\(k\\) picks up minor changes, but can produce false positives. A larger \\(k\\) is more robust to noise, but can miss faint foregrounds.\n",
    "\n",
    "4. **Foreground Updates / Online Learning:**  \n",
    "   - Real systems often **update** the background model over time, gradually incorporating new frames where no foreground is detected. This allows adaptation to slow lighting changes.\n",
    "\n",
    "5. **Connected Components vs. Contours:**  \n",
    "   - You can also use `cv2.findContours` for blob detection. Choose the method that best suits your project requirements.\n",
    "\n",
    "6. **Tracking & Counting:**  \n",
    "   - Once you have each blob’s centroid, you can match them across frames (e.g., nearest centroid match, Hungarian algorithm, or a Kalman filter) to track movement and count how many objects enter or exit a region.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "**Difference keying** involves modeling the background (mean and variance) from an “empty” scene and then classifying new pixels that deviate significantly from this model as **foreground**. This approach is conceptually simple yet effective in static or near-static scenes with steady lighting. Optional steps include refining the alpha channel for partial transparency, cleaning up the mask with morphological operations, and tracking connected components (e.g., for people counting).  \n",
    "\n",
    "**References:**\n",
    "- R. Szeliski, *Computer Vision: Algorithms and Applications*, Section 3.1.3.  \n",
    "- Toyama, K., Krumm, J., Brumitt, B., & Meyers, B. (1999). *Wallflower: Principles and practice of background maintenance*. In ICCV.\n",
    "\n",
    "With this pipeline, you can detect and track objects in your scene, laying the groundwork for more advanced video analytics tasks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Capture \"empty\" scene frames to build background model\n",
    "# -------------------------\n",
    "# For demonstration, we assume we read a few frames from a video or webcam.\n",
    "# Alternatively, this could be a set of pre-captured images.\n",
    "NUM_EMPTY_FRAMES = 30\n",
    "cap = cv2.VideoCapture('empty_scene_video.mp4')  # or 0 for a webcam\n",
    "\n",
    "frame_count = 0\n",
    "frames_list = []\n",
    "\n",
    "while frame_count < NUM_EMPTY_FRAMES:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Convert to float32\n",
    "    float_frame = frame.astype(np.float32)\n",
    "    frames_list.append(float_frame)\n",
    "    frame_count += 1\n",
    "\n",
    "if len(frames_list) == 0:\n",
    "    print(\"No frames captured for background model!\")\n",
    "    exit()\n",
    "\n",
    "# Compute mean and variance for background model\n",
    "bg_mean = np.mean(frames_list, axis=0)  # shape: (H, W, C)\n",
    "bg_var  = np.var(frames_list, axis=0)   # shape: (H, W, C)\n",
    "bg_std  = np.sqrt(bg_var)               # standard deviation\n",
    "\n",
    "# Define a scaling factor for threshold\n",
    "k = 2.5  # tune this\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Process new frames and apply difference keying\n",
    "# -------------------------\n",
    "# Let's loop over the remainder of the video to do foreground detection.\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to float32\n",
    "    float_frame = frame.astype(np.float32)\n",
    "    \n",
    "    # Compute difference from the background mean\n",
    "    diff = cv2.absdiff(float_frame, bg_mean)\n",
    "    # Optionally convert to grayscale or keep as 3D\n",
    "    # We'll take the norm across color channels for simplicity\n",
    "    diff_norm = np.sqrt(np.sum(diff**2, axis=2))\n",
    "    \n",
    "    # Compute threshold using standard deviations\n",
    "    # We only need the grayscale equivalent of bg_std if we do norm-based detection\n",
    "    bg_std_gray = np.sqrt(np.sum(bg_std**2, axis=2))  # approximate aggregated std\n",
    "    threshold_map = k * bg_std_gray\n",
    "    \n",
    "    # Create a foreground mask\n",
    "    # foreground_mask = 1 where diff_norm > threshold_map, else 0\n",
    "    fg_mask = (diff_norm > threshold_map).astype(np.uint8)\n",
    "    \n",
    "    # (Optional) Morphological cleanup\n",
    "    # Create a kernel for morphology\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    # Remove noise (Opening)\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    # Fill holes (Closing)\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # (Optional) Label connected components\n",
    "    num_labels, labels_im = cv2.connectedComponents(fg_mask)\n",
    "    \n",
    "    # Draw bounding boxes or centroids for each label\n",
    "    # We skip the background label = 0\n",
    "    output_frame = frame.copy()\n",
    "    for label_idx in range(1, num_labels):\n",
    "        mask_region = (labels_im == label_idx)\n",
    "        # Find coordinates of this blob\n",
    "        y_coords, x_coords = np.where(mask_region)\n",
    "        # Compute bounding box\n",
    "        x_min, x_max = x_coords.min(), x_coords.max()\n",
    "        y_min, y_max = y_coords.min(), y_coords.max()\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(output_frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "        \n",
    "        # Centroid\n",
    "        cx = int(np.mean(x_coords))\n",
    "        cy = int(np.mean(y_coords))\n",
    "        cv2.circle(output_frame, (cx, cy), 5, (0,0,255), -1)\n",
    "    \n",
    "    # Display results\n",
    "    cv2.imshow('Foreground Mask', fg_mask*255)\n",
    "    cv2.imshow('Labeled Objects', output_frame)\n",
    "    \n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:  # Escape key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
